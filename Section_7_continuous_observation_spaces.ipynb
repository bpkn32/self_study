{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "9vN8l4-UvgqJ"
      },
      "source": [
        "<div style=\"text-align:center\">\n",
        "    <h1>\n",
        "        Continuous state spaces\n",
        "    </h1>\n",
        "</div>\n",
        "\n",
        "<br>\n",
        "\n",
        "<div style=\"text-align:center\">\n",
        "    In this notebook we will learn how to adapt tabular methods to continuous state spaces. We will do it with two methods:\n",
        "    state aggregation and tile coding.\n",
        "</div>\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Setup code (not important) - Run this cell by pressing \"Shift + Enter\"\n",
        "\n",
        "\n",
        "\n",
        "!pip install -qq gym==0.23.0\n",
        "\n",
        "\n",
        "from typing import Tuple, Dict, Optional, Iterable, Callable\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "import torch\n",
        "from matplotlib import animation\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "import gym\n",
        "from gym import spaces\n",
        "from gym.error import DependencyNotInstalled\n",
        "\n",
        "import pygame\n",
        "from pygame import gfxdraw\n",
        "\n",
        "\n",
        "class Maze(gym.Env):\n",
        "\n",
        "    def __init__(self, exploring_starts: bool = False,\n",
        "                 shaped_rewards: bool = False, size: int = 5) -> None:\n",
        "        super().__init__()\n",
        "        self.exploring_starts = exploring_starts\n",
        "        self.shaped_rewards = shaped_rewards\n",
        "        self.state = (size - 1, size - 1)\n",
        "        self.goal = (size - 1, size - 1)\n",
        "        self.maze = self._create_maze(size=size)\n",
        "        self.distances = self._compute_distances(self.goal, self.maze)\n",
        "        self.action_space = spaces.Discrete(n=4)\n",
        "        self.action_space.action_meanings = {0: 'UP', 1: 'RIGHT', 2: 'DOWN', 3: \"LEFT\"}\n",
        "        self.observation_space = spaces.MultiDiscrete([size, size])\n",
        "\n",
        "        self.screen = None\n",
        "        self.agent_transform = None\n",
        "\n",
        "    def step(self, action: int) -> Tuple[Tuple[int, int], float, bool, Dict]:\n",
        "        reward = self.compute_reward(self.state, action)\n",
        "        self.state = self._get_next_state(self.state, action)\n",
        "        done = self.state == self.goal\n",
        "        info = {}\n",
        "        return self.state, reward, done, info\n",
        "\n",
        "    def reset(self) -> Tuple[int, int]:\n",
        "        if self.exploring_starts:\n",
        "            while self.state == self.goal:\n",
        "                self.state = tuple(self.observation_space.sample())\n",
        "        else:\n",
        "            self.state = (0, 0)\n",
        "        return self.state\n",
        "\n",
        "    def render(self, mode: str = 'human') -> Optional[np.ndarray]:\n",
        "        assert mode in ['human', 'rgb_array']\n",
        "\n",
        "        screen_size = 600\n",
        "        scale = screen_size / 5\n",
        "\n",
        "        if self.screen is None:\n",
        "            pygame.init()\n",
        "            self.screen = pygame.Surface((screen_size, screen_size))\n",
        "\n",
        "        surf = pygame.Surface((screen_size, screen_size))\n",
        "        surf.fill((22, 36, 71))\n",
        "\n",
        "\n",
        "        for row in range(5):\n",
        "            for col in range(5):\n",
        "\n",
        "                state = (row, col)\n",
        "                for next_state in [(row + 1, col), (row - 1, col), (row, col + 1), (row, col - 1)]:\n",
        "                    if next_state not in self.maze[state]:\n",
        "\n",
        "                        # Add the geometry of the edges and walls (i.e. the boundaries between\n",
        "                        # adjacent squares that are not connected).\n",
        "                        row_diff, col_diff = np.subtract(next_state, state)\n",
        "                        left = (col + (col_diff > 0)) * scale - 2 * (col_diff != 0)\n",
        "                        right = ((col + 1) - (col_diff < 0)) * scale + 2 * (col_diff != 0)\n",
        "                        top = (5 - (row + (row_diff > 0))) * scale - 2 * (row_diff != 0)\n",
        "                        bottom = (5 - ((row + 1) - (row_diff < 0))) * scale + 2 * (row_diff != 0)\n",
        "\n",
        "                        gfxdraw.filled_polygon(surf, [(left, bottom), (left, top), (right, top), (right, bottom)], (255, 255, 255))\n",
        "\n",
        "        # Add the geometry of the goal square to the viewer.\n",
        "        left, right, top, bottom = scale * 4 + 10, scale * 5 - 10, scale - 10, 10\n",
        "        gfxdraw.filled_polygon(surf, [(left, bottom), (left, top), (right, top), (right, bottom)], (40, 199, 172))\n",
        "\n",
        "        # Add the geometry of the agent to the viewer.\n",
        "        agent_row = int(screen_size - scale * (self.state[0] + .5))\n",
        "        agent_col = int(scale * (self.state[1] + .5))\n",
        "        gfxdraw.filled_circle(surf, agent_col, agent_row, int(scale * .6 / 2), (228, 63, 90))\n",
        "\n",
        "        surf = pygame.transform.flip(surf, False, True)\n",
        "        self.screen.blit(surf, (0, 0))\n",
        "\n",
        "        return np.transpose(\n",
        "                np.array(pygame.surfarray.pixels3d(self.screen)), axes=(1, 0, 2)\n",
        "            )\n",
        "\n",
        "    def close(self) -> None:\n",
        "        if self.screen is not None:\n",
        "            pygame.display.quit()\n",
        "            pygame.quit()\n",
        "            self.screen = None\n",
        "\n",
        "    def compute_reward(self, state: Tuple[int, int], action: int) -> float:\n",
        "        next_state = self._get_next_state(state, action)\n",
        "        if self.shaped_rewards:\n",
        "            return - (self.distances[next_state] / self.distances.max())\n",
        "        return - float(state != self.goal)\n",
        "\n",
        "    def simulate_step(self, state: Tuple[int, int], action: int):\n",
        "        reward = self.compute_reward(state, action)\n",
        "        next_state = self._get_next_state(state, action)\n",
        "        done = next_state == self.goal\n",
        "        info = {}\n",
        "        return next_state, reward, done, info\n",
        "\n",
        "    def _get_next_state(self, state: Tuple[int, int], action: int) -> Tuple[int, int]:\n",
        "        if action == 0:\n",
        "            next_state = (state[0] - 1, state[1])\n",
        "        elif action == 1:\n",
        "            next_state = (state[0], state[1] + 1)\n",
        "        elif action == 2:\n",
        "            next_state = (state[0] + 1, state[1])\n",
        "        elif action == 3:\n",
        "            next_state = (state[0], state[1] - 1)\n",
        "        else:\n",
        "            raise ValueError(\"Action value not supported:\", action)\n",
        "        if next_state in self.maze[state]:\n",
        "            return next_state\n",
        "        return state\n",
        "\n",
        "    @staticmethod\n",
        "    def _create_maze(size: int) -> Dict[Tuple[int, int], Iterable[Tuple[int, int]]]:\n",
        "        maze = {(row, col): [(row - 1, col), (row + 1, col), (row, col - 1), (row, col + 1)]\n",
        "                for row in range(size) for col in range(size)}\n",
        "\n",
        "        left_edges = [[(row, 0), (row, -1)] for row in range(size)]\n",
        "        right_edges = [[(row, size - 1), (row, size)] for row in range(size)]\n",
        "        upper_edges = [[(0, col), (-1, col)] for col in range(size)]\n",
        "        lower_edges = [[(size - 1, col), (size, col)] for col in range(size)]\n",
        "        walls = [\n",
        "            [(1, 0), (1, 1)], [(2, 0), (2, 1)], [(3, 0), (3, 1)],\n",
        "            [(1, 1), (1, 2)], [(2, 1), (2, 2)], [(3, 1), (3, 2)],\n",
        "            [(3, 1), (4, 1)], [(0, 2), (1, 2)], [(1, 2), (1, 3)],\n",
        "            [(2, 2), (3, 2)], [(2, 3), (3, 3)], [(2, 4), (3, 4)],\n",
        "            [(4, 2), (4, 3)], [(1, 3), (1, 4)], [(2, 3), (2, 4)],\n",
        "        ]\n",
        "\n",
        "        obstacles = upper_edges + lower_edges + left_edges + right_edges + walls\n",
        "\n",
        "        for src, dst in obstacles:\n",
        "            maze[src].remove(dst)\n",
        "\n",
        "            if dst in maze:\n",
        "                maze[dst].remove(src)\n",
        "\n",
        "        return maze\n",
        "\n",
        "    @staticmethod\n",
        "    def _compute_distances(goal: Tuple[int, int],\n",
        "                           maze: Dict[Tuple[int, int], Iterable[Tuple[int, int]]]) -> np.ndarray:\n",
        "        distances = np.full((5, 5), np.inf)\n",
        "        visited = set()\n",
        "        distances[goal] = 0.\n",
        "\n",
        "        while visited != set(maze):\n",
        "            sorted_dst = [(v // 5, v % 5) for v in distances.argsort(axis=None)]\n",
        "            closest = next(x for x in sorted_dst if x not in visited)\n",
        "            visited.add(closest)\n",
        "\n",
        "            for neighbour in maze[closest]:\n",
        "                distances[neighbour] = min(distances[neighbour], distances[closest] + 1)\n",
        "        return distances\n",
        "\n",
        "\n",
        "def plot_policy(probs_or_qvals, frame, action_meanings=None):\n",
        "    if action_meanings is None:\n",
        "        action_meanings = {0: 'U', 1: 'R', 2: 'D', 3: 'L'}\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
        "    max_prob_actions = probs_or_qvals.argmax(axis=-1)\n",
        "    probs_copy = max_prob_actions.copy().astype(object)\n",
        "    for key in action_meanings:\n",
        "        probs_copy[probs_copy == key] = action_meanings[key]\n",
        "    sns.heatmap(max_prob_actions, annot=probs_copy, fmt='', cbar=False, cmap='coolwarm',\n",
        "                annot_kws={'weight': 'bold', 'size': 12}, linewidths=2, ax=axes[0])\n",
        "    axes[1].imshow(frame)\n",
        "    axes[0].axis('off')\n",
        "    axes[1].axis('off')\n",
        "    plt.suptitle(\"Policy\", size=18)\n",
        "    plt.tight_layout()\n",
        "\n",
        "\n",
        "def plot_values(state_values, frame):\n",
        "    f, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "    sns.heatmap(state_values, annot=True, fmt=\".2f\", cmap='coolwarm',\n",
        "                annot_kws={'weight': 'bold', 'size': 12}, linewidths=2, ax=axes[0])\n",
        "    axes[1].imshow(frame)\n",
        "    axes[0].axis('off')\n",
        "    axes[1].axis('off')\n",
        "    plt.tight_layout()\n",
        "\n",
        "\n",
        "def display_video(frames):\n",
        "    # Copied from: https://colab.research.google.com/github/deepmind/dm_control/blob/master/tutorial.ipynb\n",
        "    orig_backend = matplotlib.get_backend()\n",
        "    matplotlib.use('Agg')\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
        "    matplotlib.use(orig_backend)\n",
        "    ax.set_axis_off()\n",
        "    ax.set_aspect('equal')\n",
        "    ax.set_position([0, 0, 1, 1])\n",
        "    im = ax.imshow(frames[0])\n",
        "    def update(frame):\n",
        "        im.set_data(frame)\n",
        "        return [im]\n",
        "    anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
        "                                    interval=50, blit=True, repeat=False)\n",
        "    return HTML(anim.to_html5_video())\n",
        "\n",
        "\n",
        "def test_agent(env, policy, episodes=10):\n",
        "    frames = []\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        frames.append(env.render(mode=\"rgb_array\"))\n",
        "\n",
        "        while not done:\n",
        "            p = policy(state)\n",
        "            if isinstance(p, np.ndarray):\n",
        "                action = np.random.choice(4, p=p)\n",
        "            else:\n",
        "                action = p\n",
        "            next_state, reward, done, extra_info = env.step(action)\n",
        "            img = env.render(mode=\"rgb_array\")\n",
        "            frames.append(img)\n",
        "            state = next_state\n",
        "\n",
        "    return display_video(frames)\n",
        "\n",
        "\n",
        "def plot_action_values(action_values):\n",
        "\n",
        "    text_positions = [\n",
        "        [(0.35, 4.75), (1.35, 4.75), (2.35, 4.75), (3.35, 4.75), (4.35, 4.75),\n",
        "         (0.35, 3.75), (1.35, 3.75), (2.35, 3.75), (3.35, 3.75), (4.35, 3.75),\n",
        "         (0.35, 2.75), (1.35, 2.75), (2.35, 2.75), (3.35, 2.75), (4.35, 2.75),\n",
        "         (0.35, 1.75), (1.35, 1.75), (2.35, 1.75), (3.35, 1.75), (4.35, 1.75),\n",
        "         (0.35, 0.75), (1.35, 0.75), (2.35, 0.75), (3.35, 0.75), (4.35, 0.75)],\n",
        "        [(0.6, 4.45), (1.6, 4.45), (2.6, 4.45), (3.6, 4.45), (4.6, 4.45),\n",
        "         (0.6, 3.45), (1.6, 3.45), (2.6, 3.45), (3.6, 3.45), (4.6, 3.45),\n",
        "         (0.6, 2.45), (1.6, 2.45), (2.6, 2.45), (3.6, 2.45), (4.6, 2.45),\n",
        "         (0.6, 1.45), (1.6, 1.45), (2.6, 1.45), (3.6, 1.45), (4.6, 1.45),\n",
        "         (0.6, 0.45), (1.6, 0.45), (2.6, 0.45), (3.6, 0.45), (4.6, 0.45)],\n",
        "        [(0.35, 4.15), (1.35, 4.15), (2.35, 4.15), (3.35, 4.15), (4.35, 4.15),\n",
        "         (0.35, 3.15), (1.35, 3.15), (2.35, 3.15), (3.35, 3.15), (4.35, 3.15),\n",
        "         (0.35, 2.15), (1.35, 2.15), (2.35, 2.15), (3.35, 2.15), (4.35, 2.15),\n",
        "         (0.35, 1.15), (1.35, 1.15), (2.35, 1.15), (3.35, 1.15), (4.35, 1.15),\n",
        "         (0.35, 0.15), (1.35, 0.15), (2.35, 0.15), (3.35, 0.15), (4.35, 0.15)],\n",
        "        [(0.05, 4.45), (1.05, 4.45), (2.05, 4.45), (3.05, 4.45), (4.05, 4.45),\n",
        "         (0.05, 3.45), (1.05, 3.45), (2.05, 3.45), (3.05, 3.45), (4.05, 3.45),\n",
        "         (0.05, 2.45), (1.05, 2.45), (2.05, 2.45), (3.05, 2.45), (4.05, 2.45),\n",
        "         (0.05, 1.45), (1.05, 1.45), (2.05, 1.45), (3.05, 1.45), (4.05, 1.45),\n",
        "         (0.05, 0.45), (1.05, 0.45), (2.05, 0.45), (3.05, 0.45), (4.05, 0.45)]]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(7, 7))\n",
        "    tripcolor = quatromatrix(action_values, ax=ax,\n",
        "                             triplotkw={\"color\": \"k\", \"lw\": 1}, tripcolorkw={\"cmap\": \"coolwarm\"})\n",
        "    ax.margins(0)\n",
        "    ax.set_aspect(\"equal\")\n",
        "    fig.colorbar(tripcolor)\n",
        "\n",
        "    for j, av in enumerate(text_positions):\n",
        "        for i, (xi, yi) in enumerate(av):\n",
        "            plt.text(xi, yi, round(action_values[:, :, j].flatten()[i], 2), size=8, color=\"w\", weight=\"bold\")\n",
        "\n",
        "    plt.title(\"Action values Q(s,a)\", size=18)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def quatromatrix(action_values, ax=None, triplotkw=None, tripcolorkw=None):\n",
        "    action_values = np.flipud(action_values)\n",
        "    n = 5\n",
        "    m = 5\n",
        "    a = np.array([[0, 0], [0, 1], [.5, .5], [1, 0], [1, 1]])\n",
        "    tr = np.array([[0, 1, 2], [0, 2, 3], [2, 3, 4], [1, 2, 4]])\n",
        "    A = np.zeros((n * m * 5, 2))\n",
        "    Tr = np.zeros((n * m * 4, 3))\n",
        "    for i in range(n):\n",
        "        for j in range(m):\n",
        "            k = i * m + j\n",
        "            A[k * 5:(k + 1) * 5, :] = np.c_[a[:, 0] + j, a[:, 1] + i]\n",
        "            Tr[k * 4:(k + 1) * 4, :] = tr + k * 5\n",
        "    C = np.c_[action_values[:, :, 3].flatten(), action_values[:, :, 2].flatten(),\n",
        "              action_values[:, :, 1].flatten(), action_values[:, :, 0].flatten()].flatten()\n",
        "\n",
        "    ax.triplot(A[:, 0], A[:, 1], Tr, **triplotkw)\n",
        "    tripcolor = ax.tripcolor(A[:, 0], A[:, 1], Tr, facecolors=C, **tripcolorkw)\n",
        "    return tripcolor\n",
        "\n",
        "\n",
        "def seed_everything(env: gym.Env, seed: int = 42) -> None:\n",
        "    env.seed(seed)\n",
        "    env.action_space.seed(seed)\n",
        "    env.observation_space.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.use_deterministic_algorithms(True)\n",
        "\n",
        "\n",
        "def plot_tabular_cost_to_go(action_values, xlabel, ylabel):\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    cost_to_go = -action_values.max(axis=-1)\n",
        "    plt.imshow(cost_to_go, cmap='jet')\n",
        "    plt.title(\"Estimated cost-to-go\", size=24)\n",
        "    plt.xlabel(xlabel, size=18)\n",
        "    plt.ylabel(ylabel, size=18)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.xticks()\n",
        "    plt.colorbar()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_stats(stats):\n",
        "    rows = len(stats)\n",
        "    cols = 1\n",
        "\n",
        "    fig, ax = plt.subplots(rows, cols, figsize=(12, 6))\n",
        "\n",
        "    for i, key in enumerate(stats):\n",
        "        vals = stats[key]\n",
        "        vals = [np.mean(vals[i-10:i+10]) for i in range(10, len(vals)-10)]\n",
        "        if len(stats) > 1:\n",
        "            ax[i].plot(range(len(vals)), vals)\n",
        "            ax[i].set_title(key, size=18)\n",
        "        else:\n",
        "            ax.plot(range(len(vals)), vals)\n",
        "            ax.set_title(key, size=18)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "RH8HXBTBvwBs",
        "cellView": "form",
        "outputId": "5ade8467-92c4-4a77-ea11-c526b7c9c9d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/624.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/624.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.4/624.4 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a40Bl59vgqL"
      },
      "source": [
        "## Import the necessary software libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ITOgJ74WvgqL"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYUqQBl1vgqM"
      },
      "source": [
        "## Implement state aggregation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3COnudvvgqM"
      },
      "source": [
        "### Create the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Zpo8BQEAvgqM",
        "outputId": "469f3a03-8764-421d-b115-b86c6bd26927",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]]], dtype=uint8)"
            ],
            "text/html": [
              "<style>\n",
              "      .ndarray_repr .ndarray_raw_data {\n",
              "        display: none;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_raw_data {\n",
              "        display: block;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_image_preview {\n",
              "        display: none;\n",
              "      }\n",
              "      </style>\n",
              "      <div id=\"id-c23235b4-8273-4f05-ad47-495894eb9242\" class=\"ndarray_repr\"><pre>ndarray (400, 600, 3) <button style=\"padding: 0 2px;\">show data</button></pre><img src=\"data:image/png;base64,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\" class=\"ndarray_image_preview\" /><pre class=\"ndarray_raw_data\">array([[[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]]], dtype=uint8)</pre></div><script>\n",
              "      (() => {\n",
              "      const titles = ['show data', 'hide data'];\n",
              "      let index = 0\n",
              "      document.querySelector('#id-c23235b4-8273-4f05-ad47-495894eb9242 button').onclick = (e) => {\n",
              "        document.querySelector('#id-c23235b4-8273-4f05-ad47-495894eb9242').classList.toggle('show_array');\n",
              "        index = (++index) % 2;\n",
              "        document.querySelector('#id-c23235b4-8273-4f05-ad47-495894eb9242 button').textContent = titles[index];\n",
              "        e.preventDefault();\n",
              "        e.stopPropagation();\n",
              "      }\n",
              "      })();\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "env = gym.make(\"MountainCar-v0\",render_mode = \"rgb_array\")\n",
        "env.reset()\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qo3ssx5cvgqM"
      },
      "source": [
        "### Create the state aggregation wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IVjaqy9FvgqM"
      },
      "outputs": [],
      "source": [
        "class StateAggregationEnv(gym.ObservationWrapper):\n",
        "    def __init__(self, env, bins, low, high):\n",
        "      super().__init__(env)\n",
        "      self.buckets = [np.linspace(l,h,b-1) for l,h,b in zip(low,high,bins)]\n",
        "\n",
        "      self.observation_space = gym.spaces.MultiDiscrete(nvec = bins.tolist())\n",
        "\n",
        "    def observation(self, obs):\n",
        "      indices = tuple(np.digitize(x,y) for x,y in zip(obs,self.buckets))\n",
        "      return indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GNAXqTtlvgqN"
      },
      "outputs": [],
      "source": [
        "bins = np.array([20,20])\n",
        "low = env.observation_space.low\n",
        "high = env.observation_space.high\n",
        "saenv = StateAggregationEnv(env,bins,low,high)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "J4cAudZsvgqN",
        "outputId": "f266fb1f-42b0-4f4e-899f-11aad47264e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([-1.2       , -1.1       , -1.        , -0.90000004, -0.8000001 ,\n",
              "        -0.70000005, -0.6       , -0.50000006, -0.40000004, -0.3       ,\n",
              "        -0.20000005, -0.10000002,  0.        ,  0.10000002,  0.19999993,\n",
              "         0.29999995,  0.39999998,  0.5       ,  0.6       ], dtype=float32),\n",
              " array([-0.07      , -0.06222222, -0.05444444, -0.04666667, -0.03888889,\n",
              "        -0.03111111, -0.02333333, -0.01555555, -0.00777778,  0.        ,\n",
              "         0.00777778,  0.01555556,  0.02333333,  0.03111111,  0.03888889,\n",
              "         0.04666667,  0.05444445,  0.06222222,  0.07      ], dtype=float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "saenv.buckets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGyKnzn6vgqN"
      },
      "source": [
        "### Compare the original environment to the one with aggregated states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Rd_a-ui9vgqN",
        "outputId": "42f1c571-8e8d-4535-bfd3-1d70a6d6c7a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modified observation space: MultiDiscrete([20 20]), \n",
            "Sample state: [19 10]\n"
          ]
        }
      ],
      "source": [
        "print(f\"Modified observation space: {saenv.observation_space}, \\n\\\n",
        "Sample state: {saenv.observation_space.sample()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9csRyU1SvgqN",
        "outputId": "e4435631-e473-4960-da4d-e230b4facac8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original observation space: Box([-1.2  -0.07], [0.6  0.07], (2,), float32), \n",
            "Sample state: [-1.0500699   0.01102207]\n"
          ]
        }
      ],
      "source": [
        "print(f\"Original observation space: {env.observation_space}, \\n\\\n",
        "Sample state: {env.observation_space.sample()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_igeDTqvgqN"
      },
      "source": [
        "### Create the $Q(s,a)$ value table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XmmC_IwfvgqN"
      },
      "outputs": [],
      "source": [
        "action_values = np.zeros((20,20, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxEX6ZvGvgqN"
      },
      "source": [
        "### Create the $\\epsilon$-greedy policy: $\\pi(s)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "U2CSJjIzvgqN"
      },
      "outputs": [],
      "source": [
        "def policy(state, epsilon=0.):\n",
        "    if np.random.random() < epsilon:\n",
        "        return np.random.randint(3)\n",
        "    else:\n",
        "        av = action_values[state]\n",
        "        return np.random.choice(np.flatnonzero(av == av.max()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnZ2AUVmvgqN"
      },
      "source": [
        "### Test the SARSA algorithm on the modified environment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tqdm(range(1,10))"
      ],
      "metadata": {
        "id": "xgwhYy90xHvj",
        "outputId": "f90890fb-7b86-48e3-ecc2-a7fa80c029a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/9 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tqdm.std.tqdm at 0x7eba82ac24e0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jitSZTRRvgqO"
      },
      "outputs": [],
      "source": [
        "def sarsa(action_values, policy, episodes, alpha=0.1, gamma=0.99, epsilon=0.2):\n",
        "    stats = {'Returns': []}\n",
        "    for episode in tqdm(range(1, episodes + 1)):\n",
        "        state = saenv.reset()\n",
        "        action = policy(state, epsilon)\n",
        "        done = False\n",
        "        ep_return = 0\n",
        "        while not done:\n",
        "            next_state, reward, done, _,_ = saenv.step(action)\n",
        "            next_action = policy(next_state, epsilon)\n",
        "\n",
        "            qsa = action_values[state][action]\n",
        "            next_qsa = action_values[next_state][next_action]\n",
        "            action_values[state][action] = qsa + alpha * (reward + gamma * next_qsa - qsa)\n",
        "            state = next_state\n",
        "            action = next_action\n",
        "            ep_return += reward\n",
        "        stats['Returns'].append(ep_return)\n",
        "    return stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "pRoh3U-QvgqO",
        "outputId": "149f2c57-bfec-4bc4-8f49-24708588a2b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/20000 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3963972113.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msarsa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-557808302.py\u001b[0m in \u001b[0;36msarsa\u001b[0;34m(action_values, policy, episodes, alpha, gamma, epsilon)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mep_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3975453004.py\u001b[0m in \u001b[0;36mpolicy\u001b[0;34m(state, epsilon)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mav\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mav\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
          ]
        }
      ],
      "source": [
        "stats = sarsa(action_values, policy, 20000, alpha=0.1, epsilon=0.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RacbxBmZvgqO"
      },
      "outputs": [],
      "source": [
        "plot_stats(stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZmOJxuUvgqO"
      },
      "source": [
        "### Plot the learned policy: $\\pi(s)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57NP8GQRvgqO"
      },
      "outputs": [],
      "source": [
        "plot_policy(action_values, env.render(mode='rgb_array'), \\\n",
        "            action_meanings={0: 'B', 1: 'N', 2: 'F'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nvBZFSgvgqO"
      },
      "source": [
        "### Plot the cost to go: $ - \\max_a \\hat q(s,a|\\theta)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "SUsd_F73vgqO"
      },
      "outputs": [],
      "source": [
        "plot_tabular_cost_to_go(action_values, xlabel=\"Car Position\", ylabel=\"Velocity\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1P7kUmXvgqO"
      },
      "source": [
        "### Test the resulting policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZX89VryvgqO"
      },
      "outputs": [],
      "source": [
        "test_agent(saenv, policy, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W47pYQdCvgqO"
      },
      "source": [
        "<br><br><br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVg1SvxOvgqO"
      },
      "source": [
        "## Implement Tile Coding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOggOxgHvgqO"
      },
      "source": [
        "### Create the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YD3C41lgvgqO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbbJcOtCvgqO"
      },
      "source": [
        "### Create the Tile Coding wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZls8wqNvgqO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zo286x38vgqO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJ4JBdNTvgqO"
      },
      "source": [
        "### Compare the original environment to the one with aggregated states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Bs_FL8jvgqO"
      },
      "outputs": [],
      "source": [
        "print(f\"Modified observation space: {tcenv.observation_space}, \\n\\\n",
        "Sample state: {tcenv.reset()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mg7B8yH1vgqO"
      },
      "outputs": [],
      "source": [
        "print(f\"Original observation space: {env.observation_space}, \\n\\\n",
        "Sample state: {env.reset()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hxhy2cXmvgqO"
      },
      "source": [
        "### Create the $Q(s,a)$ value table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y46HsY2TvgqS"
      },
      "outputs": [],
      "source": [
        "action_values = np.zeros((4, 20, 20, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bI9pWIevgqS"
      },
      "source": [
        "### Create the $\\epsilon$-greedy policy: $\\pi(s)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9K2iDRbIvgqS"
      },
      "outputs": [],
      "source": [
        "def policy(state, epsilon=0.):\n",
        "    if np.random.random() < epsilon:\n",
        "        return np.random.randint(3)\n",
        "    else:\n",
        "        av_list = []\n",
        "        for i, idx in enumerate(state):\n",
        "            av = action_values[i][idx]\n",
        "            av_list.append(av)\n",
        "\n",
        "        av = np.mean(av_list, axis=0)\n",
        "        return np.random.choice(np.flatnonzero(av==av.max()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRGnsmKlvgqS"
      },
      "source": [
        "### Test the SARSA algorithm on the modified environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UtvHUrxvgqS"
      },
      "outputs": [],
      "source": [
        "def sarsa(action_values, policy, episodes, alpha=0.1, gamma=0.99, epsilon=0.2):\n",
        "    stats = {'Returns': []}\n",
        "    for episode in tqdm(range(1, episodes + 1)):\n",
        "        state = tcenv.reset()\n",
        "        action = policy(state, epsilon)\n",
        "        done = False\n",
        "        ep_return = 0\n",
        "        while not done:\n",
        "            next_state, reward, done, _ = tcenv.step(action)\n",
        "            next_action = policy(next_state, epsilon)\n",
        "\n",
        "            for i, (idx, next_idx) in enumerate(zip(state, next_state)):\n",
        "                qsa = action_values[i][idx][action]\n",
        "                next_qsa = action_values[i][next_idx][next_action]\n",
        "                action_values[i][idx][action] = qsa + alpha * (reward + gamma * next_qsa - qsa)\n",
        "\n",
        "            state = next_state\n",
        "            action = next_action\n",
        "            ep_return += reward\n",
        "        stats['Returns'].append(ep_return)\n",
        "    return stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MW_884NBvgqS"
      },
      "outputs": [],
      "source": [
        "stats = sarsa(action_values, policy, 20000, alpha=0.1, epsilon=0.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGdpV9TNvgqS"
      },
      "outputs": [],
      "source": [
        "plot_stats(stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_MzBqr2vgqS"
      },
      "source": [
        "### Plot the learned policy: $\\pi(s)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jV_XpCylvgqS"
      },
      "outputs": [],
      "source": [
        "plot_policy(action_values.mean(axis=0), env.render(mode='rgb_array'), \\\n",
        "            action_meanings={0: 'B', 1: 'N', 2: 'F'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_0TY7oovgqT"
      },
      "source": [
        "### Plot the cost to go: $ - \\max_a \\hat q(s,a|\\theta)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3G9nCyMvgqT"
      },
      "outputs": [],
      "source": [
        "plot_tabular_cost_to_go(action_values.mean(axis=0), \\\n",
        "                        xlabel=\"Car Position\", ylabel=\"Velocity\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8z4PLeQvgqT"
      },
      "source": [
        "### Test the resulting policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmopjBSTvgqT"
      },
      "outputs": [],
      "source": [
        "test_agent(tcenv, policy, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_qLNpc0vgqT"
      },
      "source": [
        "## Resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8lQEV8uvgqT"
      },
      "source": [
        "[[1] Reinforcement Learning: An Introduction. Section 9.5.4: Tile Coding](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}